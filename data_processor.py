# data_processor.py
import pandas as pd
import os
import numpy as np
import re
import random
from database import BrandMapping, KnownProductName, ProductMapping

def load_product_mappings_from_db():
    """Load product weight/size mappings from database"""
    try:
        mappings = ProductMapping.query.all()
        return {m.product_name: {'weight': m.box_weight, 'size': m.box_size} for m in mappings}
    except Exception as e:
        print(f"Warning: Could not load product mappings from database: {e}")
        return {}

# DEPRECATED: No longer auto-updating from Excel files
# All weight/size data now comes exclusively from ProductMapping table
# Use rebuild_product_mapping.py to update ProductMapping from source Excel files
#
# def update_product_mappings_from_excel(df):
#     """
#     Update product mappings in database with weight/size info from Excel.
#     Only updates existing products or creates new ones if weight/size data is available.
#
#     Args:
#         df: DataFrame with columns 'å“å', 'å•ä»¶å‡€é‡(kg)', 'è§„æ ¼'
#     """
#     ...

def load_brand_mappings_from_db():
    """Load brand mappings from database"""
    try:
        mappings = BrandMapping.query.all()
        return {m.brand_name: m.reference_name for m in mappings}
    except Exception as e:
        print(f"Warning: Could not load brand mappings from database: {e}")
        # Fallback to hardcoded values if database is not available
        return {
            '072LABO': '072LABO',
            'AO': 'A-one',
            "AVS Collector's": "AVS Collector's",
            'FANTASTICBABY': 'FANTASTICBABY',
            'HP': 'Hotpowers',
            'ME': 'MagicEyes',
            'PT': 'Peach Toys',
            'RJ': 'Ride Japan',
            'TH': 'Toys Heart',
            'TMT': 'Tamatoys',
            'ã‚¢ãƒªã‚¹JAPAN': 'çˆ±ä¸½ä¸',
            'ã‚­ãƒ†ãƒ«ã‚­ãƒ†ãƒ«': 'Kiteru Kiteru',
            'ãƒãƒˆãƒ—ãƒ©': 'EXE',
            'ãƒãƒˆãƒ—ãƒ©(EXE)': 'EXE',
            'ãƒãƒˆãƒ—ãƒ©(GPRO)': 'EXE',
            'ãƒãƒˆãƒ—ãƒ©(HATOPLA)': 'EXE',
            'ãƒãƒˆãƒ—ãƒ©(PPP)': 'EXE',
        }

def load_known_names_from_db():
    """Load known product names from database"""
    try:
        names = KnownProductName.query.all()
        return [n.product_name for n in names]
    except Exception as e:
        print(f"Warning: Could not load known names from database: {e}")
        # Fallback to hardcoded values if database is not available
        return [
            'ä½“ä½DX',
            'èµ¤è²ä¹³è±†ã‚¹ã‚¯ãƒ©ãƒ–',
            'ã‚ªãƒŠãƒ›å±‹ã•ã‚“ã®ã æ¶²æ±',
            'èƒ¡å¤¢',
            'æœ‰å‚æ·±é›ª',
            'ãƒãƒ³ã‚³ãƒ„ã‚¬ãƒ¼ãƒ‡ã‚£ã‚¢ãƒ³ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ ãƒ¦ãƒ‹ã‚³ãƒ¼ãƒ³ãƒ—ãƒ¬ãƒŸã‚¢ãƒ è‚‰åšãƒ•ã‚¡ãƒ“ãƒ¥ãƒ©ã‚¹',
            'å¹¼é¦´æŸ“',
            'PUNIVIRGIN[ã·ã«ã°ãƒ¼ã˜ã‚“]1000 ãµã‚ã¨ã‚',
            'Chuï¼ï¼»ãƒãƒ¥ãƒƒï¼ï¼½',
            'JAPANESE REAL HOLE',
            'ã·ã«ã‚ãªSPDX ',
            'ã·ã«ã‚ãªãƒŸãƒ©ã‚¯ãƒ«çˆ†ä¹³DX',
            'ç¶²',
            'ã‘ã‚‚ã‚ãƒ¼ã—ã‚‡ã‚“',
            'å¯¾é­”å¿',
            'ZARA GYUGYU',
            'ã™ã˜ã¾ã‚“ ãã±ã ã‚³ã‚³ãƒ­',
            'æ¥µå½©ã‚¦ãƒ†ãƒ«ã‚¹ ',
            'ã‚ªãƒŠã‚·ãƒ¼',
            'ã‚ã‚Šã‚“ã“å‰µä¸–è¨˜ãƒŠãƒã‚¤ã‚­HARD',
            'ç–‘ä¼¼ã¡ã¤ ãƒ„ãƒ–ã³ã‚‰æ·«ã‚¹ãƒ‘ã‚¤ãƒ©ãƒ«ã€€ã™ã‘ã™ã‘',
            'æ¿€ãƒ•ã‚§ãƒ©',
            'ã¾ã‚‹å‰¥ãŒã—ãƒªã‚¢ãƒ« å…«ä¹ƒã¤ã°ã•',
            'ãƒ•ã‚§ãƒ© ãŠå£ã§ã¡ã‚…ãã—ã¾ã™',
            'åå™¨è¦šé†’',
            'ç´ äººãƒªã‚¢ãƒ«',
            'åå™¨ã®è¨¼æ˜',
            'åœ°é›·ç³»å¥³å­ãƒ‘ãƒ³ã‚­ãƒ¼ãƒ¡ãƒƒã‚·ãƒ¥',
            'ãƒãƒ­ãƒ³ã‚³ãƒ­ãƒ³',
            'æ¬²æƒ…',
            'å‡¦å¥³ãã±ã',
            'ãŠãªã¤ã‚†(onatsuyu)370ml',
            "ã‚ã»ã™ãŸã•ã‚“å°ã®æ¯ä¹³ãƒ­ãƒ¼ã‚·ãƒ§ãƒ³(Fake mother's milk Lubricant)",
            'ç¾å½¹JD',
            'åŒ‚ã„',
            'ãƒ—ãƒ«ãƒ«ãƒ³ãŠã£ã±ã„ãƒ‹ãƒ—ãƒ«ãƒ•ã‚¡ãƒƒã‚¯',
            'åƒé¶´',
            'åˆ¶æœç¾å°‘å¥³',
            'å…ˆè¼©',
            'çˆ†ä¹³',
            'ä¸‰èŠ³ã®æ„›æ¶²',
            '360 FETISH æ½®SPLASH',
            'ãƒã‚¯ãƒ‹ãƒ¼ï¼†ãƒã‚¯ãƒ‘ã‚³',
            'ONAç ²',
            'ã‚ªãƒŠãƒ›ãƒ«ãƒ€ãƒ¼',
            'AVãƒŸãƒ‹åå™¨',
            'çœŸå®Ÿã®å£',
            'ã‚ã“ãŒã‚Œã®ç¾å°‘å¥³ãƒ–ãƒ«ãƒ',
            'åå™¨å‰µç”Ÿ',
            'ç´ äººãƒªã‚¢ãƒ«',
            'æ—¥æœ¬ã®ãƒ­ãƒ¼ã‚·ãƒ§ãƒ³',
        ]

def join_unique_strings(series):
    """
    Cleans the series, finds unique non-missing values, and joins them into a single string.
    """
    # 1. Drop NaN values
    series_no_nan = series.dropna()
    
    # 2. Convert all values to strings (critical for concatenation)
    series_str = series_no_nan.astype(str)
    
    # 3. Get only unique values (to avoid repetition)
    unique_values = series_str.unique()
    
    # 4. Join them with a delimiter (e.g., comma and space)
    return ', '.join(unique_values)

def process_manufacturer_data(file_paths, mapping_config):
    """
    Reads multiple manufacturer files, cleans them, and aggregates data.
    """
    all_data = []
    
    for path in file_paths:
        try:
            # 1. Excel Parsing: Read the file into a Pandas DataFrame
            df = pd.read_excel(path)

            # 2. Data Cleaning & Transformation:

            # Standardize column names from source Excel files
            # Map 'æ—¥æ–‡åå­—' to 'å“å' if it exists
            if 'æ—¥æ–‡åå­—' in df.columns and 'å“å' not in df.columns:
                df.rename(columns={'æ—¥æ–‡åå­—': 'å“å'}, inplace=True)

            # Drop rows with missing critical data
            df.dropna(subset=['å“ç‰Œ', 'å“å'], inplace=True)
            
            # Ensure data types are correct
            df['Pcs'] = pd.to_numeric(df['Pcs'], errors='coerce')
            df['Price'] = pd.to_numeric(df['Price'], errors='coerce')
            
            all_data.append(df)
            
        except Exception as e:
            print(f"Error processing {path}: {e}")
            # Log the error and move to the next file

    # 3. Aggregation: Combine all manufacturer data
    if not all_data:
        return pd.DataFrame() # Return empty if no data
    
    master_df = pd.concat(all_data, ignore_index=True)

    # Load known product names from database
    KNOWN_NAMES = load_known_names_from_db()

    # Add all ProductMapping product names to KNOWN_NAMES for better matching coverage
    # This improves coverage from ~34% to ~59% by matching product name variants
    product_mappings = load_product_mappings_from_db()
    KNOWN_NAMES = list(set(KNOWN_NAMES) | set(product_mappings.keys()))
    print(f'ğŸ“š Combined KNOWN_NAMES: {len(KNOWN_NAMES)} names (50 original + {len(product_mappings)} from ProductMapping)')

    # Escape special characters and join with '|' for OR logic
    # (\b ensures it matches whole words, but for substring matching, it's optional)
    # The (?i) makes the pattern case-insensitive
    pattern = '(?i)(' + '|'.join(map(re.escape, KNOWN_NAMES)) + ')'

    master_df['å“å'] = (
        master_df['å“å'].str.extract(pattern, expand=True)[0].fillna(master_df['å“å'])
    )

    # 1. Define the numerical thresholds (bins)
    # Note: The first bin must be lower than your minimum price, and the last must be higher than your maximum price.
    # Use -np.inf and np.inf to catch all possible values.
    price_bins = [
        -np.inf, # Lowest possible number
        500,     # Upper bound for the first category
        1000,    # Upper bound for the second category
        2500,    # Upper bound for the third category
        5000,    # Upper bound for the 4th category
        10000,    # Upper bound for the 5th category
        20000,    # Upper bound for the 6th category
        30000,    # Upper bound for the 7th category
        np.inf   # Highest possible number
    ]
    # 2. Define the corresponding labels (must have one less label than bins)
    price_labels = [
        'less_than_500', 
        '500_to_1000', 
        '1000_to_2500', 
        '2500_to_5000',
        '5000_to_10000', 
        '10000_to_20000', 
        '20000_to_30000', 
        'over_30000'
    ]
    master_df['ä»·æ ¼åŒºé—´'] = pd.cut(
        master_df['Price'],
        bins=price_bins,
        labels=price_labels,
        right=True, # Intervals are (a, b] - means 500 goes into the '500_to_1000' category
        include_lowest=True # Ensures the lowest value in the data is captured
    )

    # Load brand mappings from database
    value_mapping = load_brand_mappings_from_db()

    master_df['å“ç‰Œ'] = (
        master_df['å“ç‰Œ']
        .map(value_mapping)
        # The crucial step: fill the NaNs (unmapped values)
        # with the corresponding values from the original column.
        .fillna(master_df['å“ç‰Œ'])
    )

    # Load product weight/size mappings from database ONLY
    # Excel files should NOT contain å•ä»¶å‡€é‡(kg) or è§„æ ¼ - all data comes from ProductMapping table
    product_mappings = load_product_mappings_from_db()

    # Create mapping functions
    def get_weight(product_name):
        mapping = product_mappings.get(product_name)
        return mapping['weight'] if mapping and mapping['weight'] is not None else None

    def get_size(product_name):
        mapping = product_mappings.get(product_name)
        return mapping['size'] if mapping and mapping['size'] is not None else None

    # Always populate å•ä»¶å‡€é‡(kg) and è§„æ ¼ from database, ignoring any Excel columns
    # This ensures we ONLY use data from ProductMapping table
    master_df['å•ä»¶å‡€é‡(kg)'] = master_df['å“å'].apply(get_weight)
    master_df['è§„æ ¼'] = master_df['å“å'].apply(get_size)

    # Calculate å‡€é‡ (net weight) = å•ä»¶å‡€é‡(kg) * Pcs
    master_df['å‡€é‡'] = master_df['å•ä»¶å‡€é‡(kg)'] * master_df['Pcs']

    # Calculate key metrics for the board
    # Use lambda with first valid (non-null) value for weight and size
    summary_df = master_df.groupby(['å“ç‰Œ', 'å“å', 'ä»·æ ¼åŒºé—´', 'åˆ†é¡','ç”£åœ°'], observed=True).agg(
        å‹å·=('å‹ç•ª', join_unique_strings),
        æ•°é‡=('Pcs', 'sum'),
        æ€»ä»·æ ¼=('Total', 'sum'),
    ).reset_index()

    # Add weight and size columns separately to use Chinese column names with parentheses
    summary_df['å•ä»¶å‡€é‡(kg)'] = master_df.groupby(['å“ç‰Œ', 'å“å', 'ä»·æ ¼åŒºé—´', 'åˆ†é¡','ç”£åœ°'], observed=True)['å•ä»¶å‡€é‡(kg)'].apply(
        lambda x: x.dropna().iloc[0] if len(x.dropna()) > 0 else None
    ).values

    summary_df['è§„æ ¼'] = master_df.groupby(['å“ç‰Œ', 'å“å', 'ä»·æ ¼åŒºé—´', 'åˆ†é¡','ç”£åœ°'], observed=True)['è§„æ ¼'].apply(
        lambda x: x.dropna().iloc[0] if len(x.dropna()) > 0 else None
    ).values

    summary_df['å‡€é‡'] = master_df.groupby(['å“ç‰Œ', 'å“å', 'ä»·æ ¼åŒºé—´', 'åˆ†é¡','ç”£åœ°'], observed=True)['å‡€é‡'].sum().values

    # Replace å‡€é‡ with None if value is 0 (means no weight data available)
    summary_df['å‡€é‡'] = summary_df['å‡€é‡'].apply(lambda x: None if x == 0 else x)

    summary_df['æ¯›é‡'] = summary_df['å‡€é‡'] * random.uniform(1.08, 1.12)

    # Build æŠ¥å…³ column with model information
    summary_df['æŠ¥å…³'] = np.where(
        (summary_df['å‹å·'].isna()) | (summary_df['å‹å·'] == ''),
        'å‹å·ï¼šæ— å‹å·',
        'å‹å·ï¼š' + summary_df['å‹å·'].astype(str)
    )

    # Add prefix for ADULT TOY category
    adult_toy_prefix = 'æˆäººç”¨å“ æˆäººè§£å†³ç”Ÿç†éœ€æ±‚ç”¨|çƒ­å¡‘æ€§å¼¹æ€§ä½“TPEåˆ¶ '
    summary_df['æŠ¥å…³'] = np.where(
        (summary_df['åˆ†é¡'] == 'ADULT TOY') | (summary_df['åˆ†é¡'] == 'ELECTRIC ADULT TOY') | (summary_df['åˆ†é¡'] == 'CLOTHING'),
        adult_toy_prefix + summary_df['æŠ¥å…³'],
        summary_df['æŠ¥å…³']
    )

    # add prefix for lotion category
    lotion_prefix = 'æ¶¦æ»‘æ¶²äººä½“æ¶¦æ»‘ç”¨|æ°´90%ï¼Œç”˜æ²¹5%ï¼Œèšä¸™çƒ¯é…¸é’ 5%|ä¸å«ä»çŸ³æ²¹æˆ–æ²¥é’æå–çŸ¿ç‰©æ²¹ç±» '
    summary_df['æŠ¥å…³'] = np.where(
        summary_df['åˆ†é¡'] == 'LOTION',
        lotion_prefix + summary_df['æŠ¥å…³'],
        summary_df['æŠ¥å…³']
    )

    return summary_df